{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e641f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install langchain-community langchain-neo4j langchainhub neo4j\n",
    "# !pip install openai langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722c11f7",
   "metadata": {},
   "source": [
    "##### Create a Langchain application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d72541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "response = llm.invoke(\"What is Neo4j?\") #LLM.Invoke'a bir soru aktarabilir ve bir yanıt alabilirsiniz.\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ac8faf",
   "metadata": {},
   "source": [
    "Neo4j, Neo4j Inc. tarafından geliştirilen açık kaynaklı, NoSQL grafik veritabanı yönetim sistemidir. Nodes, relationships ve properties kullanarak verileri grafik benzeri bir yapıda depolamak ve yönetmek için tasarlanmıştır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2dc99b",
   "metadata": {},
   "source": [
    "##### Prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7d0009b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou are a cockney fruit and vegetable seller.\\nYour role is to assist your customer with their fruit and vegetable needs.\\nRespond using cockney rhyming slang.\\n\\nTell me about the following fruit: {fruit}\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "You are a cockney fruit and vegetable seller.\n",
    "Your role is to assist your customer with their fruit and vegetable needs.\n",
    "Respond using cockney rhyming slang.\n",
    "\n",
    "Tell me about the following fruit: {fruit}\n",
    "\"\"\"\n",
    "# Bu prompt template, LLM'ye context sağlayacak ve ona meyve ve sebze satan bir satıcı olarak yanıt vermesi talimatını verecektir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ddd0fc",
   "metadata": {},
   "source": [
    "{} şablon içinde parantez kullanarak parametreler tanımlayabilirsiniz, örneğin {fruit}. Bu parametreler, prompt biçimlendirildiğinde değerlerle değiştirilecektir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b97a9dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template'i kullanmak için programımızı değiştirelim:\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(template=\"\"\"\n",
    "You are a cockney fruit and vegetable seller.\n",
    "Your role is to assist your customer with their fruit and vegetable needs.\n",
    "Respond using cockney rhyming slang.\n",
    "\n",
    "Tell me about the following fruit: {fruit}\n",
    "\"\"\", input_variables=[\"fruit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4466240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biçimlendirilmiş prompt template'i input olarak geçirerek LLM'yi çağırın(call):\n",
    "response = llm.invoke(template.format(fruit=\"apple\")) #format: Parametreleri promptta geçirmek için\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a876acb1",
   "metadata": {},
   "source": [
    "Parametreleri prompt'a iletmek için (fruit = \"elma\") format yöntemini kullanırsınız. Input variables, prompt biçimlendirildiğinde doğrulanacak ve girişte herhangi bir değişken eksikse bir KeyError verceektir.\n",
    "\n",
    "\n",
    "Prompt aşağıdaki gibi biçimlendirilecektir:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30499898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a cockney fruit and vegetable seller.\\n\\nYour role is to assist your customer with their fruit and vegetable needs\\n\\nRespond using cockney rhyming slang.\\n\\nTell me about the following fruit: apple'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"You are a cockney fruit and vegetable seller.\n",
    "\n",
    "Your role is to assist your customer with their fruit and vegetable needs\n",
    "\n",
    "Respond using cockney rhyming slang.\n",
    "\n",
    "Tell me about the following fruit: apple\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf960c3",
   "metadata": {},
   "source": [
    "Programı çalıştırırken, aşağıdakilere benzer bir yanıt görmelisiniz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "202a3a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well, apples is a right corker - they come in all shapes and sizes from Granny Smiths'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Well, apples is a right corker - they come in all shapes and sizes from Granny Smiths\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5a18e8",
   "metadata": {},
   "source": [
    "Programı defalarca çalıştırırsanız, LLM her seferinde cevabı ürettiği için farklı yanıtlar aldığınızı fark edeceksiniz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6a214e",
   "metadata": {},
   "source": [
    "PromptTemplates Oluşturma\n",
    "\n",
    "PromptTemplate.from_template() static method çağırarak bir stringden bir prompt oluşturabilir veya PromptTemplate.from_file() static method kullanarak bir dosyadan bir prompt yükleyebilirsiniz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bfd2e5",
   "metadata": {},
   "source": [
    "##### Configuring the LLM (LLM'i Yapılandırma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d796d04f",
   "metadata": {},
   "source": [
    "LLM'yi oluşturduğunuzda, temperature ve model gibi parametrelerle yapılandırabilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0265df",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18b5266",
   "metadata": {},
   "source": [
    "Tüm promptların bir temperature değeri vardır. 0.0 ile 1.0 arasındadır ve yanıtın rastgeleliğini veya yaratıcılığını etkiler.\n",
    "Değer 0'a yakın oldukça daha net yanıtlar sağlarken 1'e yaklaştıkça daha yaratıcı yanıtlar üretir ancak bu durum halliculation'a sebep olabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3862445",
   "metadata": {},
   "source": [
    "Prompt Templates, yeniden kullanılabilir talimatlar oluşturmanıza olanak tanır.\n",
    "\n",
    "Prompt Templates, birden fazla parametreyi kabul edebilir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550476e6",
   "metadata": {},
   "source": [
    "##### Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367e6957",
   "metadata": {},
   "source": [
    "LCEL\n",
    "\n",
    "LangChain Expression Language (LCEL) kullanarak bir chain oluşturabilirsiniz. LCEL, Langchain componentlerini birlikte chainlemenin bir yoludur.\n",
    "Componentler \"|\" operatorünü kullanarak birbirine zincirlenir(chain olur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16c3566",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997682f0",
   "metadata": {},
   "source": [
    "Daha önce, meyve hakkında bir yanıt oluşturmak için hızlı bir şablon ve bir LLM kullanan bir program oluşturmuştuk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c11ba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate#\n",
    "\n",
    "llm = OpenAI(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "template = PromptTemplate(template=\"\"\"\n",
    "You are a cockney fruit and vegetable seller.\n",
    "Your role is to assist your customer with their fruit and vegetable needs.\n",
    "Respond using cockney rhyming slang.\n",
    "\n",
    "Tell me about the following fruit: {fruit}\n",
    "\"\"\", input_variables=[\"fruit\"])\n",
    "\n",
    "response = llm.invoke(template.format(fruit=\"apple\"))\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa7f8a2",
   "metadata": {},
   "source": [
    "Bu programı bir chain ile birleştirebilir ve bir reusable component(yeniden kullanılabilir component) oluşturabiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b42e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"\n",
    "You are a cockney fruit and vegetable seller.\n",
    "Your role is to assist your customer with their fruit and vegetable needs.\n",
    "Respond using cockney rhyming slang.\n",
    "\n",
    "Tell me about the following fruit: {fruit}\n",
    "\"\"\")\n",
    "\n",
    "llm_chain = template | llm\n",
    "\n",
    "response = llm_chain.invoke({\"fruit\": \"apple\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1df08d0",
   "metadata": {},
   "source": [
    "llm_chain'in template ve llm'i chainleyerek nasıl oluşturulduğuna dikkat edin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157f58a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = template | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef1ab3c",
   "metadata": {},
   "source": [
    "Template parametrelerini bir dictionary olarak geçirerek(invoke) llm_chain'i çağırırız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f5ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_chain.invoke({\"fruit\": \"apple\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061ae086",
   "metadata": {},
   "source": [
    "##### Output Parsers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0f9971",
   "metadata": {},
   "source": [
    "Chain'den gelen output genellikle bir string'dir ve outputu ayrıştırmak için bir output parser belirtebilirsiniz.\n",
    "Chain'e bir StrOutputParser eklemek string olmasını sağlayacaktır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7a0fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "llm_chain = template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8835197d",
   "metadata": {},
   "source": [
    "LLM'e belirli bir output türünü döndürme talimatı vermek için promptu değiştirebilirsiniz. Örneğin, Output JSON belirterek JSON'u döndürebilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21494e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = PromptTemplate.from_template(\"\"\"\n",
    "You are a cockney fruit and vegetable seller.\n",
    "Your role is to assist your customer with their fruit and vegetable needs.\n",
    "Respond using cockney rhyming slang.\n",
    "\n",
    "Output JSON as {{\"description\": \"your response here\"}}\n",
    "\n",
    "Tell me about the following fruit: {fruit}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a650fcf",
   "metadata": {},
   "source": [
    " SimpleJsonOutputParser'ı output_parser olarak belirterek Langchain'in yanıtı JSON olarak ayrıştırmasını sağlayabilirsiniz:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0624d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "llm_chain = template | llm | SimpleJsonOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb92a3e",
   "metadata": {},
   "source": [
    "##### Chains kullanmanın faydaları:\n",
    "\n",
    "Modularity(Modülerlik): Langchain, dil modeli uygulamaları oluşturmak için kullanılabilecek birçok modül sağlar. Bu modüller basit uygulamalarda tek başına (stand-alones) kullanılabilir ve daha karmaşık kullanım durumları için birleştirilebilir.\n",
    "\n",
    "Customizability(Özelleştirilebilirlik): Çoğu Langchain uygulaması, LLM ve/veya kullanılan promptu yapılandırmanıza izin verir, bu nedenle bundan nasıl yararlanacağını bilmek büyük bir kolaylaştırıcı olacaktır.\n",
    "\n",
    "Ease of Use:(Kullanım kolaylığı): Componentler, Langchain frameworkünün geri kalanını kullanıp kullanmadığınıza bakılmaksızın kullanımı kolay olacak şekilde tasarlanmıştır.\n",
    "\n",
    "Standard Interface(Standart Arabirim): LangChain, geliştiricilerin tek bir LLM çağrısının ötesine geçen çağrı dizileri oluşturmasını sağlayan zincirler için standart bir arayüz sağlar.\n",
    "\n",
    "\n",
    "\n",
    "Chains, modüler ve yeniden kullanılabilir bileşenler oluşturmak için kullanılır.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
