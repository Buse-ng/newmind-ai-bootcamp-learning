{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4720188",
   "metadata": {},
   "source": [
    "# Chat Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d09ae",
   "metadata": {},
   "source": [
    "LLM ile sohbet etmek için bir chat modelin nasıl kullanılacağını öğreneceğiz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60793ab2",
   "metadata": {},
   "source": [
    "##### Chat Models vs Language Models\n",
    "Şimdiye kadar LLM ile iletişim kurmak için bir language model kullanıyorduk. Bir language model, bir kelime dizisindeki bir sonraki kelimeyi tahmin eder. Chat models, ise konuşmalar yapmak için tasarlanmıştır - bir mesaj listesi kabul eder ve bir konuşma yanıtı döndürürler.(they accept a list of messages and return a conversational response.)\n",
    "\n",
    "Chat models genellikle farklı messages türlerini destekler:\n",
    "\n",
    "System - LLM'e insan mesajlarına nasıl tepki vereceğini öğretir\n",
    "\n",
    "Human - kullanıcı tarafından gönderilen mesajlardır\n",
    "\n",
    "AI - AI'dan gelen yanıtlar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d79e00",
   "metadata": {},
   "source": [
    "##### Create a Chat Model\n",
    "\n",
    "Chat model kullanan bir uygulama oluşturacağız.\n",
    "\n",
    "The application will:\n",
    "\n",
    "Use a system message to provide instructions\n",
    "\n",
    "Use a human message to ask a question\n",
    "\n",
    "Receive an AI response to the question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95d7609",
   "metadata": {},
   "source": [
    "Yeni bir Python programı oluşturun, gerekli LangChain modüllerini içe aktarın ve sohbet modelini örneklendirin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9a30f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage  \n",
    "\n",
    "chat_llm = ChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd8edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SystemMessage classını kullanarak chat modele talimatlar sağlamak için bir system message oluşturalım.\n",
    "instructions = SystemMessage(content=\"\"\"\n",
    "You are a surfer dude, having a conversation about the surf conditions on the beach.\n",
    "Respond using surfer slang.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fc5f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bir soru sormak için HumanMessage nesnesini oluşturalım.\n",
    "\n",
    "question = HumanMessage(content=\"What is the weather like?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4821f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bu örnekte, sistem mesajını instructions ve insan mesajını question ile ilettik.\n",
    "\n",
    "response = chat_llm.invoke([\n",
    "    instructions,\n",
    "    question\n",
    "])\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b221c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The response is an AIMessage object.\n",
    "\n",
    "AIMessage(content=\"Dude, the weather is totally gnarly! It's sunny with some epic offshore winds. Perfect conditions for shredding some sick waves!\", additional_kwargs={}, example=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad965e8a",
   "metadata": {},
   "source": [
    "##### Wrapping in a Chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbc8a20",
   "metadata": {},
   "source": [
    "Prompt ve chains hakkında öğrendiklerimizi kullanarak yeniden kullanılabilir bir chat modeli chaini oluşturabiliriz. Chat modele bir message list aktarmak yerine, konuşmaya context veren ve sonra soruyu chat modele aktaran bir prompt oluşturabiliriz.\n",
    "\n",
    "\n",
    "Prompt, system ve human mesajlarının birleştirilmesiyle oluşturulur.\n",
    "\n",
    "Chain, chat model, prompt ve output parser kullanılarak oluşturulur.\n",
    "\n",
    "Question, invoke metodunun bir parametresi olarak chat modele iletilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae55f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "chat_llm = ChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a surfer dude, having a conversation about the surf conditions on the beach. Respond using surfer slang.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\", \n",
    "            \"{question}\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "response = chat_chain.invoke({\"question\": \"What is the weather like?\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f0b593",
   "metadata": {},
   "source": [
    "\n",
    "Bir chain oluşturmak, daha sofistike bir chat model oluşturmanın ilk adımıdır. Farklı öğeleri bir çağrıya birleştirmek ve daha karmaşık özellikleri desteklemek için chainleri kullanabiliriz.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e72009e",
   "metadata": {},
   "source": [
    "#### Giving context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0cc35e",
   "metadata": {},
   "source": [
    "Daha önce, groundingi ve LLM'e nasıl context sağlayabileceğini ve halüsinasyondan kaçınabileceğini öğrendik.\n",
    "\n",
    "Şu anda, chat modelimiz grounding yapılmadı. Question'a ve LLM'in eğitim verilerine dayanarak (aylar veya yıllar güncel olabilir) yanıt verir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f59e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "chat_llm = ChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a surfer dude, having a conversation about the surf conditions on the beach. Respond using surfer slang.\",\n",
    "        ),\n",
    "        ( \"system\", \"{context}\" ),\n",
    "        ( \"human\", \"{question}\" ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "current_weather = \"\"\"\n",
    "    {\n",
    "        \"surf\": [\n",
    "            {\"beach\": \"Fistral\", \"conditions\": \"6ft waves and offshore winds\"},\n",
    "            {\"beach\": \"Polzeath\", \"conditions\": \"Flat and calm\"},\n",
    "            {\"beach\": \"Watergate Bay\", \"conditions\": \"3ft waves and onshore winds\"}\n",
    "        ]\n",
    "    }\"\"\"\n",
    "\n",
    "response = chat_chain.invoke(\n",
    "    {\n",
    "        \"context\": current_weather,\n",
    "        \"question\": \"What is the weather like on Watergate Bay?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4861ec87",
   "metadata": {},
   "source": [
    "Aşağıda tipik bir yanıt var. LLM, daha doğru bir yanıt sağlamak için istemde iletilen bağlamı kullanmıştır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15dbff7",
   "metadata": {},
   "source": [
    "Dude, the surf at Watergate Bay is pumping! We got some sick 3ft waves rolling in, but unfortunately, we got some onshore winds messing with the lineup. But hey, it's all good, still plenty of stoke to be had out there!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081fb031",
   "metadata": {},
   "source": [
    "Context sağlamak, RAG'in bir yönüdür. Bu programda, model contextini manuel olarak verdik; Ancak, bir API veya veritabanından real-time bilgi alabilirdik.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885817ff",
   "metadata": {},
   "source": [
    "#### Giving a Chat Model Memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e540c7fd",
   "metadata": {},
   "source": [
    "Bir sohbet modelinin yardımcı olması için hangi mesajların gönderildiğini ve alındığını hatırlaması gerekir.\n",
    "\n",
    "Sohbet modelini hatırlama yeteneği olmadan konuşma bağlamına göre hareket edemez.\n",
    "\n",
    "Örneğin, bir hafıza olmadan konuşma daireler halinde gidebilir:\n",
    "\n",
    "[user] Hi, my name is Martin\n",
    "\n",
    "[chat model] Hi, nice to meet you Martin\n",
    "\n",
    "[user] Do you have a name?\n",
    "\n",
    "[chat model] I am the chat model. Nice to meet you. What is your name?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc97efa",
   "metadata": {},
   "source": [
    "#### Conversation Memory - Konuşma belleği\n",
    "\n",
    "Önceki derste, kullanıcının sorularına yanıt veren bir sohbet modeli oluşturduk. Bu derste, bu programa bir bellek ekleyeceğiz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18963b6c",
   "metadata": {},
   "source": [
    "Langchain, farklı senaryoları ve depolama çözümlerini destekleyen çeşitli memory componentleri destekler.\n",
    "\n",
    "Chat model ile aramızdaki konuşma geçmişini geçici olarak depolamak için ChatMessageHistory memory componenti kullanacağız."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7be19a9",
   "metadata": {},
   "source": [
    "#### Add History to the Prompt\n",
    "\n",
    "As each call to the LLM is stateless, you need to include the chat history in every call to the LLM. You can modify the prompt template to include the chat history as a list of messages using a MessagesPlaceholder object.\n",
    "\n",
    "LLM'ye yapılan her call stateless olduğundan, her call'da sohbet geçmişini eklememiz gerekir. Prompt template'i, sohbet geçmişini bir MessagePlaceholder nesnesi kullanarak mesajların bir listesi olarak ekleyecek şekilde değiştirebilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304625b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a surfer dude, having a conversation about the surf conditions on the beach. Respond using surfer slang.\",\n",
    "        ),\n",
    "        (\"system\", \"{context}\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chat_history değişkeni konuşma geçmişini içerecektir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac86d6f4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
